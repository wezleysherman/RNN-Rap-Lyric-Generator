{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Rap_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vRLu_X4g0Y6",
        "colab_type": "text"
      },
      "source": [
        "# RNN Rap Generator\n",
        "\n",
        "### Author: Wezley Sherman\n",
        "\n",
        "### Email: wezleysherman@yahoo.com\n",
        "\n",
        "### Medium: https://medium.com/@dawezdog\n",
        "\n",
        "### GitHub: https://github.com/wezleysherman/\n",
        "\n",
        "\n",
        "\n",
        "The purpose of this project is to compare four different models for ghost writing. Each model uses a different type of architecture to generate rap lyrics. The architectures consist of: SimpleRNN, GRU, LSTM, and CNN + LSTM \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "Copyright 2020 Wezley Sherman\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## References:\n",
        "\n",
        "\n",
        " * [\"Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting\"](https://arxiv.org/pdf/1612.03205.pdf) by Peter Potash, Alexey Romanov, Anna Rumshisky\n",
        "\n",
        " * [\"Song Lyrics\" Dataset](https://www.kaggle.com/paultimothymooney/poetry\n",
        ") by Paul Mooney\n",
        "\n",
        "* [Recurrent Neural Networks](https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html) By O'Reilly\n",
        "\n",
        "* [A C-LSTM Neural Network for Text Classification](https://arxiv.org/pdf/1511.08630.pdf) by Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C.M. Lau\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "The first thing I want to do is check out which GPU Colab Pro assigned me. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jXKTsuty6wK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "326bfd74-8510-4dcd-9235-e4b66e8d9dd4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 26 00:04:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNac_lH0Hx0m",
        "colab_type": "text"
      },
      "source": [
        "For this project I'm using a few libraries that aren't already included in Google Colab.\n",
        "\n",
        "These include:\n",
        "\n",
        "* [Pronouncing](https://pypi.org/project/pronouncing/) - Used for calculating rhyme index\n",
        "* [Kaggle](https://pypi.org/project/kaggle/) - Used for obtaining the dataset of artist lyrics\n",
        "* [Markovify](https://pypi.org/project/markovify/) - Used to create the base markov model for generating base lyrics\n",
        "* [Textstat](https://pypi.org/project/textstat/) - Used to calculate the readability score for a bar\n",
        "\n",
        "Then allow the user to upload their Kaggle API Token. This'll be used for fetching the dataset of"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnM1EvuLNm9p",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "outputId": "c77b39de-e5eb-4875-d048-e2e23a0caf87"
      },
      "source": [
        "!pip3 install pronouncing\n",
        "!pip3 install kaggle\n",
        "!pip3 install markovify\n",
        "!pip3 install textstat\n",
        "!pip3 install better-profanity\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/cf/4d24ac4f3ea5a57406a690ad7c07023c310185eac99adae7473c9ebdf550/cmudict-0.4.4-py2.py3-none-any.whl (938kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 6.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6223 sha256=9bdb69586f36cb742d579af5dc088ebc4601c6293c2155ab65135ac0c9d0ea07\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.4 pronouncing-0.2.0\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Collecting markovify\n",
            "  Downloading https://files.pythonhosted.org/packages/31/02/7ff79feeaaf67b9a4e01019ff5845213300d743858ad82dfc8852783c2d6/markovify-0.8.2.tar.gz\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 6.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.8.2-cp36-none-any.whl size=18261 sha256=6f2187f9b82f62d42a68cab569c091f4eeafdb2dc4808d68c8780a5c77d6dcfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3f/cc/c2750c71a820928e12f9609ff3a99d7b2c0d93eb61b7170189\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.8.2 unidecode-1.1.1\n",
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/af/0623a6e3adbcfda0be827664eacab5e02cd0a08d36f00013cb53784917a9/textstat-0.6.2-py3-none-any.whl (102kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.0MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 18.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.9.5 textstat-0.6.2\n",
            "Collecting better-profanity\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/a9/cfcd658b95e1acb51f8c100870398ae1de038d832c02410bc0214d6caf90/better_profanity-0.6.1-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hInstalling collected packages: better-profanity\n",
            "Successfully installed better-profanity-0.6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a848d3ac-06af-4d3d-8687-ea03b72a6d2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a848d3ac-06af-4d3d-8687-ea03b72a6d2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"twoface262\",\"key\":\"a8327396767ad788c0de67736657d0d4\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RoPdNLnL6Um",
        "colab_type": "text"
      },
      "source": [
        "After installing the needed libraries and uploading the Kaggle API key, the dataset will be downloaded and unzipped. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YvGgWPXtOZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a023c555-322f-49c6-d1e6-d06131d2c56e"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d paultimothymooney/poetry\n",
        "!unzip poetry.zip\n",
        "!ls\n",
        "!rm kaggle.json\n",
        "!rm ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading poetry.zip to /content\n",
            "\r  0% 0.00/2.00M [00:00<?, ?B/s]\n",
            "\r100% 2.00M/2.00M [00:00<00:00, 65.8MB/s]\n",
            "Archive:  poetry.zip\n",
            "  inflating: Kanye_West.txt          \n",
            "  inflating: Lil_Wayne.txt           \n",
            "  inflating: adele.txt               \n",
            "  inflating: al-green.txt            \n",
            "  inflating: alicia-keys.txt         \n",
            "  inflating: amy-winehouse.txt       \n",
            "  inflating: beatles.txt             \n",
            "  inflating: bieber.txt              \n",
            "  inflating: bjork.txt               \n",
            "  inflating: blink-182.txt           \n",
            "  inflating: bob-dylan.txt           \n",
            "  inflating: bob-marley.txt          \n",
            "  inflating: britney-spears.txt      \n",
            "  inflating: bruce-springsteen.txt   \n",
            "  inflating: bruno-mars.txt          \n",
            "  inflating: cake.txt                \n",
            "  inflating: dickinson.txt           \n",
            "  inflating: disney.txt              \n",
            "  inflating: dj-khaled.txt           \n",
            "  inflating: dolly-parton.txt        \n",
            "  inflating: dr-seuss.txt            \n",
            "  inflating: drake.txt               \n",
            "  inflating: eminem.txt              \n",
            "  inflating: janisjoplin.txt         \n",
            "  inflating: jimi-hendrix.txt        \n",
            "  inflating: johnny-cash.txt         \n",
            "  inflating: joni-mitchell.txt       \n",
            "  inflating: kanye-west.txt          \n",
            "  inflating: kanye.txt               \n",
            "  inflating: lady-gaga.txt           \n",
            "  inflating: leonard-cohen.txt       \n",
            "  inflating: lil-wayne.txt           \n",
            "  inflating: lin-manuel-miranda.txt  \n",
            "  inflating: lorde.txt               \n",
            "  inflating: ludacris.txt            \n",
            "  inflating: michael-jackson.txt     \n",
            "  inflating: missy-elliott.txt       \n",
            "  inflating: nickelback.txt          \n",
            "  inflating: nicki-minaj.txt         \n",
            "  inflating: nirvana.txt             \n",
            "  inflating: notorious-big.txt       \n",
            "  inflating: notorious_big.txt       \n",
            "  inflating: nursery_rhymes.txt      \n",
            "  inflating: patti-smith.txt         \n",
            "  inflating: paul-simon.txt          \n",
            "  inflating: prince.txt              \n",
            "  inflating: r-kelly.txt             \n",
            "  inflating: radiohead.txt           \n",
            "  inflating: rihanna.txt             \n",
            "adele.txt\t       drake.txt\t       michael-jackson.txt\n",
            "al-green.txt\t       dr-seuss.txt\t       missy-elliott.txt\n",
            "alicia-keys.txt        eminem.txt\t       nickelback.txt\n",
            "amy-winehouse.txt      janisjoplin.txt\t       nicki-minaj.txt\n",
            "beatles.txt\t       jimi-hendrix.txt        nirvana.txt\n",
            "bieber.txt\t       johnny-cash.txt\t       notorious_big.txt\n",
            "bjork.txt\t       joni-mitchell.txt       notorious-big.txt\n",
            "blink-182.txt\t       kaggle.json\t       nursery_rhymes.txt\n",
            "bob-dylan.txt\t       kanye.txt\t       patti-smith.txt\n",
            "bob-marley.txt\t       kanye-west.txt\t       paul-simon.txt\n",
            "britney-spears.txt     Kanye_West.txt\t       poetry.zip\n",
            "bruce-springsteen.txt  lady-gaga.txt\t       prince.txt\n",
            "bruno-mars.txt\t       leonard-cohen.txt       radiohead.txt\n",
            "cake.txt\t       lil-wayne.txt\t       rihanna.txt\n",
            "dickinson.txt\t       Lil_Wayne.txt\t       r-kelly.txt\n",
            "disney.txt\t       lin-manuel-miranda.txt  sample_data\n",
            "dj-khaled.txt\t       lorde.txt\n",
            "dolly-parton.txt       ludacris.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6OqgeZrgvGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, GRU, Conv1D, Embedding, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from better_profanity import profanity\n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random as rand\n",
        "import numpy as np\n",
        "import pronouncing\n",
        "import markovify\n",
        "import textstat\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewzlrGx8Mk5N",
        "colab_type": "text"
      },
      "source": [
        "Load an artist's lyric file, remove the byte order mask, and split the file up into an array of bars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi0jXP0obrUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "profanity.load_censor_words()\n",
        "\n",
        "artist_file = \"drake.txt\"\n",
        "artist_lyrics = []\n",
        "\n",
        "with open(artist_file, \"r\") as file:\n",
        "  song = profanity.censor(file.read())\n",
        "  artist_lyrics = song.replace('\\ufeff', '').split(\"\\n\")\n",
        "\n",
        "print(artist_lyrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5_xCiW6M5Al",
        "colab_type": "text"
      },
      "source": [
        "Create the Markov model that will be used for generating the initial first words for our rap generator.\n",
        "\n",
        "The Markov model is used in this case to ensure the first set of words for each bar has some coherence before feeding them into the neural network to generate the rest of the bar.\n",
        "\n",
        "A good reference for how Markov chains work can be [found here](https://techeffigytutorials.blogspot.com/2015/01/markov-chains-explained.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T1N90gC4t-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "markov_model = markovify.NewlineText(str(\"\\n\".join(artist_lyrics)), well_formed=False, state_size=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAFU70aXHgTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d9f02635-d7f4-4d92-c526-3f62b18a522a"
      },
      "source": [
        "sentence = markov_model.make_sentence(tries=100)\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "# Test out the readability index\n",
        "print(textstat.automated_readability_index(sentence))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After all the things that I commit to\n",
            "0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kN78cHCPq59",
        "colab_type": "text"
      },
      "source": [
        "In order to create the training data, the lyrics need to be tokenized.\n",
        "\n",
        "Tokenizing allows for the bars to be represented as matricies that correspond to what words are used in a bar. \n",
        "\n",
        "For example, if we were to tokenize the following sentences:\n",
        "* **\"Wezley is cool\"**\n",
        "* **\"You are cool\"**\n",
        "* **\"Tensorflow is very cool\"**\n",
        "\n",
        "The following sequences would be produced:\n",
        "* **[1, 2, 3]**\n",
        "* **[4, 5, 3]**\n",
        "* **[6, 2, 7, 3]**\n",
        "\n",
        "Where the word dictionary is:\n",
        "\n",
        "**['Wezley' : 1, 'is' : 2, 'cool' : 3, 'You' : 4, 'are' : 5, 'Tensorflow' : 6, 'very' : 7]**\n",
        "\n",
        "As-is, these sequences can't be fed into a model since they are of different lengths. To fix this, we add padding to the front of the arrays.\n",
        "\n",
        "With padding we get:\n",
        "* **[0, 1, 2, 3]**\n",
        "* **[0, 4, 5, 3]**\n",
        "* **[6, 2, 7, 3]**\n",
        "\n",
        "This allows us to have a standarized length for all the bars in a song. \n",
        "\n",
        "The padding is added to the front of each sequence so that the model does not forget the sequence earlier in the bar.\n",
        "\n",
        "With the padding added to each sequence, the data is split into train_X and train_y. \n",
        "\n",
        "Where: \n",
        "* **train_X** consists of a full bar minus the last word.\n",
        "* **train_y** consists of the last word in a bar\n",
        "\n",
        "The data is split like this so that the model can be trained to generate the next word in the sequence for a given bar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c9JMWM0p-wD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24d9ec61-190e-4cbd-8904-b64ca47edc1b"
      },
      "source": [
        "# Train datasets\n",
        "sequences = artist_lyrics\n",
        "\n",
        "# Tokenize for TensorFlow\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "V = len(tokenizer.word_index)+1\n",
        "\n",
        "# Padd the sequences\n",
        "seq = pad_sequences(tokenizer.texts_to_sequences(sequences), maxlen=30)\n",
        "\n",
        "# Split to X/y data\n",
        "train_X, train_y = seq[:, :-1], tf.keras.utils.to_categorical(seq[:, -1], num_classes=V)\n",
        "\n",
        "print(train_X.shape, train_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4772, 29) (4772, 3769)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9wqm9s37rBU",
        "colab_type": "text"
      },
      "source": [
        "#SimpleRNN Architecture\n",
        "\n",
        "The SimpleRNN gives an advantage by being able to look back into a sequence of data. The downfall of the SimpleRNN is that it is more susceptible to the vanishing gradient problem, which makes it unable to look far back into a sequence. \n",
        "\n",
        "\n",
        "The Simple RNN can be expressed as:\n",
        "\n",
        "$$\n",
        "h_{(t)} = f(h_{(t-1)}, x_{(1)})\n",
        "$$\n",
        "<br>\n",
        "\n",
        "Where $h(t)$ is the hidden state at a given time $t$. This function states that the current hidden state is a function of the previous hidden state and the current input.\n",
        "\n",
        "Taking this a step further, the SimpleRNN can be broken down as:\n",
        "<br><br>\n",
        "\n",
        "$$\n",
        "h_{(t)} = tanh(b_{hidden}+W_{hidden}h_{(t-1)}+W_{input}x_{(t)})\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "* $b_{hidden}$ is the hidden bias\n",
        "* $W_{hidden}$ is the hidden-to-hidden weights\n",
        "* $W_{input}$ is the hidden-to-input weights\n",
        "<br>\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "Taking a look at the code below, notice the variable $D$. <br>\n",
        "$D$ is a hyperparameter that denotes the output dimension of the embedding layer. More information can be [found here](https://www.tensorflow.org/tutorials/text/word_embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKqt-55Gs51Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06721519-a6ff-4d8d-c1c5-e44de67c7737"
      },
      "source": [
        "D = 512\n",
        "\n",
        "#Simple RNN\n",
        "T = train_X.shape[1]\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V, D)(i)\n",
        "x = Dropout(0.2)(x)\n",
        "x = SimpleRNN(150)(x)\n",
        "x = Dense(V, activation=\"softmax\")(x)\n",
        "rnn_model = Model(i, x)\n",
        "rnn_model.summary()\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "rnn_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
        "\n",
        "rnn_r = rnn_model.fit(train_X, train_y, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 29)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 29, 512)           1929728   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 29, 512)           0         \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 150)               99450     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3769)              569119    \n",
            "=================================================================\n",
            "Total params: 2,598,297\n",
            "Trainable params: 2,598,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 7.0376 - accuracy: 0.0497\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 5.9331 - accuracy: 0.0803\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 5.1303 - accuracy: 0.1865\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 4.2852 - accuracy: 0.2988\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 3.5040 - accuracy: 0.3965\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 2.8163 - accuracy: 0.4960\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 2.2248 - accuracy: 0.6150\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 1.7230 - accuracy: 0.7320\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 1.2934 - accuracy: 0.8334\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 0.9373 - accuracy: 0.9034\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 0.6672 - accuracy: 0.9459\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.4719 - accuracy: 0.9627\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.3461 - accuracy: 0.9713\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.2638 - accuracy: 0.9763\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.2055 - accuracy: 0.9778\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 0.1657 - accuracy: 0.9809\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.1415 - accuracy: 0.9826\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.1209 - accuracy: 0.9826\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.1059 - accuracy: 0.9824\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0962 - accuracy: 0.9824\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0906 - accuracy: 0.9832\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 0.0814 - accuracy: 0.9824\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0771 - accuracy: 0.9830\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0710 - accuracy: 0.9841\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0678 - accuracy: 0.9834\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0642 - accuracy: 0.9830\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 0.0634 - accuracy: 0.9818\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 6s 39ms/step - loss: 0.0604 - accuracy: 0.9820\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0575 - accuracy: 0.9843\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0567 - accuracy: 0.9834\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0548 - accuracy: 0.9837\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0545 - accuracy: 0.9822\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0521 - accuracy: 0.9841\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0524 - accuracy: 0.9828\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0503 - accuracy: 0.9826\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0504 - accuracy: 0.9822\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0497 - accuracy: 0.9826\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 0.0467 - accuracy: 0.9832\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 0.0468 - accuracy: 0.9826\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0471 - accuracy: 0.9822\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0459 - accuracy: 0.9837\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0469 - accuracy: 0.9816\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0459 - accuracy: 0.9816\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0447 - accuracy: 0.9837\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0445 - accuracy: 0.9828\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0445 - accuracy: 0.9824\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0425 - accuracy: 0.9837\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0435 - accuracy: 0.9830\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 6s 38ms/step - loss: 0.0432 - accuracy: 0.9837\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 6s 37ms/step - loss: 0.0424 - accuracy: 0.9826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEn0szDEYjOJ",
        "colab_type": "text"
      },
      "source": [
        "# Gated Recurrent Unit Architecture\n",
        "\n",
        "The GRU gives us an advantage over the SimpleRNN by having the ability to remember further down a sequence. The GRU accomplishes this by using two gates which decide the probability of retaining and forgetting previous state information.\n",
        "\n",
        "A single GRU unit can be expressed as:\n",
        "\n",
        "$$\n",
        "z_{(t)}=\\sigma(W_{xz}x_{(t)} + W_{hz}h_{(t-1)} + b_z)\n",
        "$$\n",
        "\n",
        "$$\n",
        "r_{(t)}=\\sigma(W_{xr}x_{(t)} + W_{hr}h_{(t-1)} + b_r)\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_{(t)} = (1 - z_{(t)}) * h_{(t-1)} + z_{(t)} * tanh(W_{xh}x_{(t)} + W_{hh}(r_{(t)}*h_{(t-1)}) + b_h) \n",
        "$$\n",
        "\n",
        "Where:\n",
        "* $W_{xz}$ is the input-to-hidden weights for the update gate\n",
        "* $W_{hz}$ is the hidden-to-hidden weights for the update gate\n",
        "* $b_z$ is the hidden bias for the update gate\n",
        "* $W_{xr}$ is the input-to-hidden weights for the reset gate\n",
        "* $W_{hr}$ is the hidden-to-hidden weights for the reset gate\n",
        "* $b_r$ is the hidden bias for the reset gate\n",
        "* $W_{xh}$ is the input-to-hidden weights \n",
        "* $W_{hh}$ is the output weights\n",
        "* $b_h$ is the hidden-to-output bias\n",
        "\n",
        "<br>\n",
        "\n",
        "The GRU differs from the SimpleRNN, with the introduction of a reset $r(t)$ and update $z(t)$ gate. These gates control which parts of the sequence the GRU retains and forgets for the next part of the sequence. This allows the GRU to carry information further through a sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x6XHJ5_s7DF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c7dd9c9-0af7-4de5-e5c9-192b80c1f218"
      },
      "source": [
        "# GRU\n",
        "D = 512\n",
        "\n",
        "T = train_X.shape[1]\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V + 1, D)(i)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Bidirectional(GRU(150))(x)\n",
        "x = Dense(V, activation=\"softmax\")(x)\n",
        "gru_model = Model(i, x)\n",
        "gru_model.summary()\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "gru_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
        "\n",
        "gru_r = gru_model.fit(train_X, train_y, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 29)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 29, 512)           1930240   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 29, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 300)               597600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3769)              1134469   \n",
            "=================================================================\n",
            "Total params: 3,662,309\n",
            "Trainable params: 3,662,309\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 6.9651 - accuracy: 0.0593\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 5.4387 - accuracy: 0.1624\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 4.3320 - accuracy: 0.2802\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.4585 - accuracy: 0.3841\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 2.7479 - accuracy: 0.4753\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 2.1550 - accuracy: 0.5660\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 1.6626 - accuracy: 0.6871\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 1.2588 - accuracy: 0.7915\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.9255 - accuracy: 0.8655\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.6720 - accuracy: 0.9155\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.4944 - accuracy: 0.9417\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.3640 - accuracy: 0.9602\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.2714 - accuracy: 0.9702\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.2099 - accuracy: 0.9778\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1660 - accuracy: 0.9809\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1407 - accuracy: 0.9811\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1174 - accuracy: 0.9824\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0999 - accuracy: 0.9818\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0923 - accuracy: 0.9813\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0835 - accuracy: 0.9822\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0761 - accuracy: 0.9834\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0713 - accuracy: 0.9826\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0701 - accuracy: 0.9816\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0641 - accuracy: 0.9839\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0624 - accuracy: 0.9837\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0607 - accuracy: 0.9828\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0581 - accuracy: 0.9813\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0566 - accuracy: 0.9828\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0541 - accuracy: 0.9843\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0546 - accuracy: 0.9830\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0546 - accuracy: 0.9826\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0510 - accuracy: 0.9839\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0484 - accuracy: 0.9834\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0480 - accuracy: 0.9843\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0476 - accuracy: 0.9820\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0487 - accuracy: 0.9837\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0462 - accuracy: 0.9828\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0466 - accuracy: 0.9834\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0451 - accuracy: 0.9834\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0445 - accuracy: 0.9845\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.0452 - accuracy: 0.9820\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0470 - accuracy: 0.9818\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0421 - accuracy: 0.9841\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0441 - accuracy: 0.9841\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.0438 - accuracy: 0.9847\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0441 - accuracy: 0.9824\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.0431 - accuracy: 0.9828\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0428 - accuracy: 0.9832\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0434 - accuracy: 0.9834\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0435 - accuracy: 0.9816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxm5RlRifMzG",
        "colab_type": "text"
      },
      "source": [
        "# Long Short Term Memory Architecture\n",
        "\n",
        "The LSTM takes the cake in my opinion. The LSTM is better than the SimpleRNN by being able to look further back into a sequence and better than the GRU by being able to handle even longer sequences. With that said, the LSTM has more parameters and thus takes longer to train. \n",
        "\n",
        "The LSTM works by utilizing three gates and a cell state. \n",
        "\n",
        "Here is the expression for an LSTM Cell:\n",
        "\n",
        "$$\n",
        "f_{(t)} = \\sigma (W_{xf}x_{(t)}+W_{hf}h_{(t-1)} + b_f)\n",
        "$$\n",
        "\n",
        "$$\n",
        "i_{(t)} = \\sigma (W_{xi}x_{(t)}+W_{hi}h_{(t-1)} + b_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "o_{(t)} = \\sigma (W_{xo}x_{(t)}+W_{ho}h_{(t-1)} + b_o)\n",
        "$$\n",
        "\n",
        "$$\n",
        "c_{(t)} = f_{(t)} * c_{(t-1)} + i_{(t)} * tanh(W_{xc}x_{(t)} + W_{hc}h_{(t-1)} + b_c)\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_{(t)} = o_{(t)} * tanh(c_{(t)})\n",
        "$$\n",
        "\n",
        "\n",
        "The LSTM is comprised of three different gates:\n",
        "* $i_{(t)}$ is the input gate, and controls which parts of the SimpleRNN term's output to add to the state\n",
        "* $o_{(t)}$ is the output gate, and controls which parts of the cell state should be at the output $h_{(t)}$\n",
        "* $f_{(t)}$ is the forget gate, and controls which parts of the cell state to forget\n",
        "\n",
        "\n",
        "These gates are used to control which parts of the sequence our LSTM will retain/forget.\n",
        "\n",
        "The weights are denoted if are input-to-hidden ($W_x$) or hidden-to-hidden ($W_h$) by their subscripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0aomRses7x0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a257f00-2979-467d-a682-0ed117197835"
      },
      "source": [
        "# LSTM\n",
        "D = 768\n",
        "\n",
        "T = train_X.shape[1]\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V + 1, D)(i)\n",
        "x = Dropout(0.1)(x)\n",
        "x = LSTM(100, return_sequences=True)(x)\n",
        "x = LSTM(100)(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(V, activation=\"softmax\")(x)\n",
        "lstm_model = Model(i, x)\n",
        "lstm_model.summary()\n",
        "\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "lstm_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
        "\n",
        "lstm_r = lstm_model.fit(train_X, train_y, epochs=120)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        [(None, 29)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_15 (Embedding)     (None, 29, 768)           2895360   \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 29, 768)           0         \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (None, 29, 100)           347600    \n",
            "_________________________________________________________________\n",
            "lstm_26 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3769)              380669    \n",
            "=================================================================\n",
            "Total params: 3,704,029\n",
            "Trainable params: 3,704,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/120\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 7.0797 - accuracy: 0.0451\n",
            "Epoch 2/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 6.2651 - accuracy: 0.0509\n",
            "Epoch 3/120\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 6.0466 - accuracy: 0.0513\n",
            "Epoch 4/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 5.8520 - accuracy: 0.0560\n",
            "Epoch 5/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 5.6199 - accuracy: 0.0648\n",
            "Epoch 6/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 5.3867 - accuracy: 0.0769\n",
            "Epoch 7/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 5.1439 - accuracy: 0.0960\n",
            "Epoch 8/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 4.9027 - accuracy: 0.1251\n",
            "Epoch 9/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 4.6922 - accuracy: 0.1521\n",
            "Epoch 10/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 4.4785 - accuracy: 0.1894\n",
            "Epoch 11/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 4.2797 - accuracy: 0.2150\n",
            "Epoch 12/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 4.0854 - accuracy: 0.2393\n",
            "Epoch 13/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 3.8939 - accuracy: 0.2701\n",
            "Epoch 14/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 3.7088 - accuracy: 0.3091\n",
            "Epoch 15/120\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 3.5219 - accuracy: 0.3370\n",
            "Epoch 16/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 3.3414 - accuracy: 0.3617\n",
            "Epoch 17/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 3.1604 - accuracy: 0.3965\n",
            "Epoch 18/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.9938 - accuracy: 0.4214\n",
            "Epoch 19/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.8187 - accuracy: 0.4541\n",
            "Epoch 20/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.6632 - accuracy: 0.4858\n",
            "Epoch 21/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 2.5061 - accuracy: 0.5178\n",
            "Epoch 22/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.3506 - accuracy: 0.5572\n",
            "Epoch 23/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.2107 - accuracy: 0.5872\n",
            "Epoch 24/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.0648 - accuracy: 0.6201\n",
            "Epoch 25/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.9272 - accuracy: 0.6570\n",
            "Epoch 26/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.7832 - accuracy: 0.6947\n",
            "Epoch 27/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.6633 - accuracy: 0.7293\n",
            "Epoch 28/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.5383 - accuracy: 0.7586\n",
            "Epoch 29/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.4232 - accuracy: 0.7881\n",
            "Epoch 30/120\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 1.3258 - accuracy: 0.8156\n",
            "Epoch 31/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.2173 - accuracy: 0.8355\n",
            "Epoch 32/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.1168 - accuracy: 0.8600\n",
            "Epoch 33/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 1.0278 - accuracy: 0.8720\n",
            "Epoch 34/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.9378 - accuracy: 0.8852\n",
            "Epoch 35/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.8613 - accuracy: 0.8977\n",
            "Epoch 36/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.7919 - accuracy: 0.9111\n",
            "Epoch 37/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.7223 - accuracy: 0.9204\n",
            "Epoch 38/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.6623 - accuracy: 0.9262\n",
            "Epoch 39/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.6093 - accuracy: 0.9340\n",
            "Epoch 40/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.5564 - accuracy: 0.9409\n",
            "Epoch 41/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.5103 - accuracy: 0.9455\n",
            "Epoch 42/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.4607 - accuracy: 0.9489\n",
            "Epoch 43/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.4234 - accuracy: 0.9573\n",
            "Epoch 44/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.3886 - accuracy: 0.9577\n",
            "Epoch 45/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.3615 - accuracy: 0.9587\n",
            "Epoch 46/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.3252 - accuracy: 0.9635\n",
            "Epoch 47/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.3072 - accuracy: 0.9648\n",
            "Epoch 48/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.2820 - accuracy: 0.9681\n",
            "Epoch 49/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.2591 - accuracy: 0.9734\n",
            "Epoch 50/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.2374 - accuracy: 0.9717\n",
            "Epoch 51/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2248 - accuracy: 0.9742\n",
            "Epoch 52/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2044 - accuracy: 0.9746\n",
            "Epoch 53/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1895 - accuracy: 0.9755\n",
            "Epoch 54/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1857 - accuracy: 0.9767\n",
            "Epoch 55/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1712 - accuracy: 0.9763\n",
            "Epoch 56/120\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 0.1564 - accuracy: 0.9786\n",
            "Epoch 57/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1479 - accuracy: 0.9782\n",
            "Epoch 58/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1379 - accuracy: 0.9795\n",
            "Epoch 59/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1351 - accuracy: 0.9793\n",
            "Epoch 60/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1302 - accuracy: 0.9797\n",
            "Epoch 61/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1170 - accuracy: 0.9809\n",
            "Epoch 62/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1134 - accuracy: 0.9801\n",
            "Epoch 63/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1166 - accuracy: 0.9793\n",
            "Epoch 64/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1055 - accuracy: 0.9818\n",
            "Epoch 65/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1064 - accuracy: 0.9809\n",
            "Epoch 66/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1079 - accuracy: 0.9805\n",
            "Epoch 67/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1148 - accuracy: 0.9813\n",
            "Epoch 68/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0962 - accuracy: 0.9826\n",
            "Epoch 69/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0886 - accuracy: 0.9811\n",
            "Epoch 70/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0796 - accuracy: 0.9824\n",
            "Epoch 71/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0785 - accuracy: 0.9820\n",
            "Epoch 72/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0724 - accuracy: 0.9834\n",
            "Epoch 73/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0680 - accuracy: 0.9834\n",
            "Epoch 74/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0656 - accuracy: 0.9828\n",
            "Epoch 75/120\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 0.0648 - accuracy: 0.9834\n",
            "Epoch 76/120\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 0.0641 - accuracy: 0.9828\n",
            "Epoch 77/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0616 - accuracy: 0.9834\n",
            "Epoch 78/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0629 - accuracy: 0.9834\n",
            "Epoch 79/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0600 - accuracy: 0.9830\n",
            "Epoch 80/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0617 - accuracy: 0.9822\n",
            "Epoch 81/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0611 - accuracy: 0.9826\n",
            "Epoch 82/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0631 - accuracy: 0.9834\n",
            "Epoch 83/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0703 - accuracy: 0.9828\n",
            "Epoch 84/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0688 - accuracy: 0.9824\n",
            "Epoch 85/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0689 - accuracy: 0.9805\n",
            "Epoch 86/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0930 - accuracy: 0.9793\n",
            "Epoch 87/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0856 - accuracy: 0.9778\n",
            "Epoch 88/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0731 - accuracy: 0.9803\n",
            "Epoch 89/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0609 - accuracy: 0.9822\n",
            "Epoch 90/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0515 - accuracy: 0.9824\n",
            "Epoch 91/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0492 - accuracy: 0.9826\n",
            "Epoch 92/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0481 - accuracy: 0.9811\n",
            "Epoch 93/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0471 - accuracy: 0.9832\n",
            "Epoch 94/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0455 - accuracy: 0.9820\n",
            "Epoch 95/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0456 - accuracy: 0.9843\n",
            "Epoch 96/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0451 - accuracy: 0.9832\n",
            "Epoch 97/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0445 - accuracy: 0.9837\n",
            "Epoch 98/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0428 - accuracy: 0.9843\n",
            "Epoch 99/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0413 - accuracy: 0.9858\n",
            "Epoch 100/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0421 - accuracy: 0.9851\n",
            "Epoch 101/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0433 - accuracy: 0.9837\n",
            "Epoch 102/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0425 - accuracy: 0.9830\n",
            "Epoch 103/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0440 - accuracy: 0.9832\n",
            "Epoch 104/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0453 - accuracy: 0.9830\n",
            "Epoch 105/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0742 - accuracy: 0.9790\n",
            "Epoch 106/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1294 - accuracy: 0.9667\n",
            "Epoch 107/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1000 - accuracy: 0.9728\n",
            "Epoch 108/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0581 - accuracy: 0.9824\n",
            "Epoch 109/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0542 - accuracy: 0.9830\n",
            "Epoch 110/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0455 - accuracy: 0.9837\n",
            "Epoch 111/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0420 - accuracy: 0.9834\n",
            "Epoch 112/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0439 - accuracy: 0.9830\n",
            "Epoch 113/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0414 - accuracy: 0.9853\n",
            "Epoch 114/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0412 - accuracy: 0.9824\n",
            "Epoch 115/120\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0413 - accuracy: 0.9832\n",
            "Epoch 116/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0412 - accuracy: 0.9849\n",
            "Epoch 117/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0391 - accuracy: 0.9841\n",
            "Epoch 118/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0418 - accuracy: 0.9830\n",
            "Epoch 119/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0392 - accuracy: 0.9832\n",
            "Epoch 120/120\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.0395 - accuracy: 0.9830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veEtKWSSg9L2",
        "colab_type": "text"
      },
      "source": [
        "# CNN + LSTM Architecture\n",
        "\n",
        "This one was a thought experiment that involved using two one-dimensional convolutional layer's as feature extraction before feeding them into the LSTM. The hope was that the CNN layers would allow the LSTM to learn certain features of an artist's style and help with the generation of the next word in the sequence.\n",
        "\n",
        "This model seemed to generate the most coherent lyrics, in my opinion.\n",
        "\n",
        "There was a pretty cool paper on the use of a C-LSTM architecture for text classification, I've linked it in the references section at the top of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZzy8b_Zs8is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ac223d3-5f45-4a24-c61a-65bc20e037e8"
      },
      "source": [
        "# CNN+LSTM\n",
        "D = 512\n",
        "\n",
        "T = train_X.shape[1]\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V + 1, D)(i)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Conv1D(filters=512, kernel_size=15)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Conv1D(filters=256, kernel_size=8)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Bidirectional(LSTM(250))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(V, activation=\"softmax\")(x)\n",
        "cnn_model = Model(i, x)\n",
        "cnn_model.summary()\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "cnn_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
        "\n",
        "cnn_r = cnn_model.fit(train_X, train_y, epochs=220)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 29)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 29, 512)           1930240   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 29, 512)           0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 15, 512)           3932672   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 15, 512)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 8, 256)            1048832   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 500)               1014000   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3769)              1888269   \n",
            "=================================================================\n",
            "Total params: 9,814,013\n",
            "Trainable params: 9,814,013\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 7.1742 - accuracy: 0.0474\n",
            "Epoch 2/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 6.4393 - accuracy: 0.0486\n",
            "Epoch 3/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 6.3568 - accuracy: 0.0505\n",
            "Epoch 4/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 6.3143 - accuracy: 0.0488\n",
            "Epoch 5/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 6.2788 - accuracy: 0.0557\n",
            "Epoch 6/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 6.2175 - accuracy: 0.0585\n",
            "Epoch 7/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 6.1344 - accuracy: 0.0616\n",
            "Epoch 8/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 5.9890 - accuracy: 0.0725\n",
            "Epoch 9/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 5.8228 - accuracy: 0.0807\n",
            "Epoch 10/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 5.6559 - accuracy: 0.0861\n",
            "Epoch 11/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 5.4878 - accuracy: 0.0949\n",
            "Epoch 12/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 5.3232 - accuracy: 0.1044\n",
            "Epoch 13/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 5.1674 - accuracy: 0.1169\n",
            "Epoch 14/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 5.0164 - accuracy: 0.1356\n",
            "Epoch 15/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.8705 - accuracy: 0.1521\n",
            "Epoch 16/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.7298 - accuracy: 0.1710\n",
            "Epoch 17/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.6032 - accuracy: 0.1804\n",
            "Epoch 18/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.4780 - accuracy: 0.1928\n",
            "Epoch 19/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.3524 - accuracy: 0.2077\n",
            "Epoch 20/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 4.2270 - accuracy: 0.2293\n",
            "Epoch 21/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.1201 - accuracy: 0.2446\n",
            "Epoch 22/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 4.0022 - accuracy: 0.2626\n",
            "Epoch 23/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.8992 - accuracy: 0.2827\n",
            "Epoch 24/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.7895 - accuracy: 0.2974\n",
            "Epoch 25/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.6898 - accuracy: 0.3171\n",
            "Epoch 26/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.6013 - accuracy: 0.3324\n",
            "Epoch 27/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.5022 - accuracy: 0.3456\n",
            "Epoch 28/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.4050 - accuracy: 0.3619\n",
            "Epoch 29/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.3077 - accuracy: 0.3812\n",
            "Epoch 30/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.2163 - accuracy: 0.4019\n",
            "Epoch 31/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 3.1323 - accuracy: 0.4120\n",
            "Epoch 32/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 3.0384 - accuracy: 0.4290\n",
            "Epoch 33/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.9570 - accuracy: 0.4453\n",
            "Epoch 34/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 2.8700 - accuracy: 0.4684\n",
            "Epoch 35/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.7941 - accuracy: 0.4853\n",
            "Epoch 36/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.7053 - accuracy: 0.4994\n",
            "Epoch 37/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.6217 - accuracy: 0.5176\n",
            "Epoch 38/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.5507 - accuracy: 0.5281\n",
            "Epoch 39/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.4764 - accuracy: 0.5499\n",
            "Epoch 40/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.4024 - accuracy: 0.5606\n",
            "Epoch 41/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 2.3238 - accuracy: 0.5865\n",
            "Epoch 42/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.2491 - accuracy: 0.6025\n",
            "Epoch 43/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.1783 - accuracy: 0.6226\n",
            "Epoch 44/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.1130 - accuracy: 0.6291\n",
            "Epoch 45/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 2.0330 - accuracy: 0.6467\n",
            "Epoch 46/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.9685 - accuracy: 0.6647\n",
            "Epoch 47/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.8999 - accuracy: 0.6746\n",
            "Epoch 48/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.8312 - accuracy: 0.6976\n",
            "Epoch 49/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.7879 - accuracy: 0.6968\n",
            "Epoch 50/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.7144 - accuracy: 0.7188\n",
            "Epoch 51/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 1.6474 - accuracy: 0.7376\n",
            "Epoch 52/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.6079 - accuracy: 0.7378\n",
            "Epoch 53/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.5423 - accuracy: 0.7573\n",
            "Epoch 54/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.4808 - accuracy: 0.7710\n",
            "Epoch 55/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.4328 - accuracy: 0.7783\n",
            "Epoch 56/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.3778 - accuracy: 0.7883\n",
            "Epoch 57/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.3345 - accuracy: 0.7953\n",
            "Epoch 58/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.2846 - accuracy: 0.8070\n",
            "Epoch 59/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.2286 - accuracy: 0.8208\n",
            "Epoch 60/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.1877 - accuracy: 0.8294\n",
            "Epoch 61/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.1403 - accuracy: 0.8370\n",
            "Epoch 62/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.1024 - accuracy: 0.8460\n",
            "Epoch 63/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.0490 - accuracy: 0.8527\n",
            "Epoch 64/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 1.0200 - accuracy: 0.8533\n",
            "Epoch 65/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.9795 - accuracy: 0.8623\n",
            "Epoch 66/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.9424 - accuracy: 0.8722\n",
            "Epoch 67/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.8949 - accuracy: 0.8745\n",
            "Epoch 68/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.8636 - accuracy: 0.8858\n",
            "Epoch 69/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.8267 - accuracy: 0.8877\n",
            "Epoch 70/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.8014 - accuracy: 0.8902\n",
            "Epoch 71/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.7777 - accuracy: 0.8944\n",
            "Epoch 72/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.7358 - accuracy: 0.8996\n",
            "Epoch 73/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.7030 - accuracy: 0.9086\n",
            "Epoch 74/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.6815 - accuracy: 0.9118\n",
            "Epoch 75/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.6530 - accuracy: 0.9162\n",
            "Epoch 76/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.6274 - accuracy: 0.9179\n",
            "Epoch 77/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.6027 - accuracy: 0.9229\n",
            "Epoch 78/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.5801 - accuracy: 0.9260\n",
            "Epoch 79/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.5574 - accuracy: 0.9241\n",
            "Epoch 80/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.5434 - accuracy: 0.9315\n",
            "Epoch 81/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.5140 - accuracy: 0.9359\n",
            "Epoch 82/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.4921 - accuracy: 0.9380\n",
            "Epoch 83/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.4742 - accuracy: 0.9403\n",
            "Epoch 84/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.4617 - accuracy: 0.9415\n",
            "Epoch 85/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.4335 - accuracy: 0.9497\n",
            "Epoch 86/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.4171 - accuracy: 0.9491\n",
            "Epoch 87/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.4093 - accuracy: 0.9491\n",
            "Epoch 88/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.3910 - accuracy: 0.9537\n",
            "Epoch 89/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.3706 - accuracy: 0.9549\n",
            "Epoch 90/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.3649 - accuracy: 0.9535\n",
            "Epoch 91/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.3510 - accuracy: 0.9575\n",
            "Epoch 92/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.3339 - accuracy: 0.9579\n",
            "Epoch 93/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.3175 - accuracy: 0.9598\n",
            "Epoch 94/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.3095 - accuracy: 0.9617\n",
            "Epoch 95/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.2972 - accuracy: 0.9621\n",
            "Epoch 96/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.2878 - accuracy: 0.9625\n",
            "Epoch 97/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.2697 - accuracy: 0.9673\n",
            "Epoch 98/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.2600 - accuracy: 0.9686\n",
            "Epoch 99/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.2562 - accuracy: 0.9688\n",
            "Epoch 100/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.2449 - accuracy: 0.9692\n",
            "Epoch 101/220\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.2394 - accuracy: 0.9667\n",
            "Epoch 102/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.2338 - accuracy: 0.9705\n",
            "Epoch 103/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.2227 - accuracy: 0.9686\n",
            "Epoch 104/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.2115 - accuracy: 0.9715\n",
            "Epoch 105/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.2027 - accuracy: 0.9744\n",
            "Epoch 106/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1944 - accuracy: 0.9732\n",
            "Epoch 107/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1897 - accuracy: 0.9734\n",
            "Epoch 108/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1836 - accuracy: 0.9742\n",
            "Epoch 109/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1773 - accuracy: 0.9749\n",
            "Epoch 110/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1709 - accuracy: 0.9753\n",
            "Epoch 111/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1649 - accuracy: 0.9753\n",
            "Epoch 112/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1605 - accuracy: 0.9784\n",
            "Epoch 113/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1576 - accuracy: 0.9776\n",
            "Epoch 114/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1529 - accuracy: 0.9763\n",
            "Epoch 115/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1471 - accuracy: 0.9765\n",
            "Epoch 116/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1433 - accuracy: 0.9763\n",
            "Epoch 117/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1312 - accuracy: 0.9818\n",
            "Epoch 118/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1328 - accuracy: 0.9786\n",
            "Epoch 119/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1266 - accuracy: 0.9776\n",
            "Epoch 120/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1254 - accuracy: 0.9797\n",
            "Epoch 121/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1194 - accuracy: 0.9801\n",
            "Epoch 122/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1167 - accuracy: 0.9795\n",
            "Epoch 123/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1152 - accuracy: 0.9801\n",
            "Epoch 124/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1117 - accuracy: 0.9809\n",
            "Epoch 125/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1057 - accuracy: 0.9801\n",
            "Epoch 126/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1087 - accuracy: 0.9799\n",
            "Epoch 127/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1046 - accuracy: 0.9820\n",
            "Epoch 128/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0999 - accuracy: 0.9809\n",
            "Epoch 129/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0964 - accuracy: 0.9826\n",
            "Epoch 130/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0917 - accuracy: 0.9826\n",
            "Epoch 131/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0901 - accuracy: 0.9816\n",
            "Epoch 132/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0895 - accuracy: 0.9805\n",
            "Epoch 133/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0878 - accuracy: 0.9816\n",
            "Epoch 134/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0867 - accuracy: 0.9841\n",
            "Epoch 135/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0803 - accuracy: 0.9820\n",
            "Epoch 136/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0810 - accuracy: 0.9826\n",
            "Epoch 137/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0797 - accuracy: 0.9839\n",
            "Epoch 138/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0768 - accuracy: 0.9830\n",
            "Epoch 139/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0789 - accuracy: 0.9832\n",
            "Epoch 140/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0763 - accuracy: 0.9828\n",
            "Epoch 141/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0767 - accuracy: 0.9828\n",
            "Epoch 142/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0714 - accuracy: 0.9843\n",
            "Epoch 143/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0708 - accuracy: 0.9811\n",
            "Epoch 144/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0674 - accuracy: 0.9843\n",
            "Epoch 145/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0679 - accuracy: 0.9822\n",
            "Epoch 146/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0675 - accuracy: 0.9839\n",
            "Epoch 147/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0691 - accuracy: 0.9824\n",
            "Epoch 148/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0652 - accuracy: 0.9813\n",
            "Epoch 149/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0636 - accuracy: 0.9832\n",
            "Epoch 150/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0654 - accuracy: 0.9824\n",
            "Epoch 151/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0604 - accuracy: 0.9847\n",
            "Epoch 152/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0631 - accuracy: 0.9826\n",
            "Epoch 153/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0606 - accuracy: 0.9851\n",
            "Epoch 154/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0595 - accuracy: 0.9839\n",
            "Epoch 155/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0556 - accuracy: 0.9847\n",
            "Epoch 156/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0572 - accuracy: 0.9841\n",
            "Epoch 157/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0574 - accuracy: 0.9820\n",
            "Epoch 158/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0555 - accuracy: 0.9832\n",
            "Epoch 159/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0541 - accuracy: 0.9828\n",
            "Epoch 160/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0551 - accuracy: 0.9822\n",
            "Epoch 161/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0540 - accuracy: 0.9830\n",
            "Epoch 162/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0546 - accuracy: 0.9839\n",
            "Epoch 163/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0537 - accuracy: 0.9841\n",
            "Epoch 164/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0510 - accuracy: 0.9845\n",
            "Epoch 165/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0544 - accuracy: 0.9826\n",
            "Epoch 166/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0514 - accuracy: 0.9837\n",
            "Epoch 167/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0514 - accuracy: 0.9818\n",
            "Epoch 168/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0505 - accuracy: 0.9824\n",
            "Epoch 169/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0484 - accuracy: 0.9845\n",
            "Epoch 170/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0473 - accuracy: 0.9830\n",
            "Epoch 171/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0483 - accuracy: 0.9828\n",
            "Epoch 172/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0484 - accuracy: 0.9832\n",
            "Epoch 173/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0482 - accuracy: 0.9847\n",
            "Epoch 174/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0481 - accuracy: 0.9830\n",
            "Epoch 175/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0470 - accuracy: 0.9837\n",
            "Epoch 176/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0481 - accuracy: 0.9834\n",
            "Epoch 177/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0455 - accuracy: 0.9839\n",
            "Epoch 178/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0454 - accuracy: 0.9841\n",
            "Epoch 179/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0447 - accuracy: 0.9847\n",
            "Epoch 180/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0455 - accuracy: 0.9832\n",
            "Epoch 181/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0477 - accuracy: 0.9813\n",
            "Epoch 182/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0448 - accuracy: 0.9832\n",
            "Epoch 183/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0441 - accuracy: 0.9834\n",
            "Epoch 184/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0447 - accuracy: 0.9843\n",
            "Epoch 185/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0464 - accuracy: 0.9832\n",
            "Epoch 186/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0450 - accuracy: 0.9839\n",
            "Epoch 187/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0427 - accuracy: 0.9841\n",
            "Epoch 188/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0426 - accuracy: 0.9858\n",
            "Epoch 189/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0430 - accuracy: 0.9841\n",
            "Epoch 190/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0425 - accuracy: 0.9830\n",
            "Epoch 191/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0425 - accuracy: 0.9845\n",
            "Epoch 192/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0438 - accuracy: 0.9834\n",
            "Epoch 193/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0442 - accuracy: 0.9830\n",
            "Epoch 194/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0430 - accuracy: 0.9853\n",
            "Epoch 195/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0415 - accuracy: 0.9834\n",
            "Epoch 196/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0401 - accuracy: 0.9832\n",
            "Epoch 197/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0395 - accuracy: 0.9843\n",
            "Epoch 198/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0418 - accuracy: 0.9826\n",
            "Epoch 199/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0400 - accuracy: 0.9853\n",
            "Epoch 200/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0410 - accuracy: 0.9834\n",
            "Epoch 201/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0423 - accuracy: 0.9826\n",
            "Epoch 202/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0413 - accuracy: 0.9830\n",
            "Epoch 203/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0403 - accuracy: 0.9832\n",
            "Epoch 204/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0390 - accuracy: 0.9870\n",
            "Epoch 205/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0412 - accuracy: 0.9830\n",
            "Epoch 206/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0392 - accuracy: 0.9839\n",
            "Epoch 207/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0405 - accuracy: 0.9837\n",
            "Epoch 208/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0429 - accuracy: 0.9826\n",
            "Epoch 209/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0415 - accuracy: 0.9824\n",
            "Epoch 210/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0395 - accuracy: 0.9841\n",
            "Epoch 211/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0410 - accuracy: 0.9839\n",
            "Epoch 212/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0400 - accuracy: 0.9832\n",
            "Epoch 213/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0410 - accuracy: 0.9824\n",
            "Epoch 214/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0404 - accuracy: 0.9830\n",
            "Epoch 215/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0391 - accuracy: 0.9834\n",
            "Epoch 216/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0398 - accuracy: 0.9839\n",
            "Epoch 217/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0415 - accuracy: 0.9837\n",
            "Epoch 218/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0398 - accuracy: 0.9832\n",
            "Epoch 219/220\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.0393 - accuracy: 0.9837\n",
            "Epoch 220/220\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.0364 - accuracy: 0.9851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59rrdxFDQsSx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e173dd38-ed37-4baf-ca5f-a704a268b712"
      },
      "source": [
        "plt.plot(rnn_r.history['loss'], label=\"SimpleRNN Loss\")\n",
        "plt.plot(gru_r.history['loss'], label=\"GRU Loss\")\n",
        "plt.plot(lstm_r.history['loss'], label=\"LSTM Loss\")\n",
        "plt.plot(cnn_r.history['loss'], label=\"CNN+LSTM Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6816b6bc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koLq2QIUjbW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "814d402c-13ba-44af-a79d-e27121ecf839"
      },
      "source": [
        "plt.plot(rnn_r.history['accuracy'], label=\"SimpleRNN accuracy\")\n",
        "plt.plot(gru_r.history['accuracy'], label=\"GRU accuracy\")\n",
        "plt.plot(lstm_r.history['accuracy'], label=\"LSTM accuracy\")\n",
        "plt.plot(cnn_r.history['accuracy'], label=\"CNN+LSTM accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f68d91ac748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIevfVdXGYof",
        "colab_type": "text"
      },
      "source": [
        "# Generating A Rap\n",
        "\n",
        "Raps are generated by using a markov model to generate the first three words of a bar.\n",
        "\n",
        "Those three words are fed into the given model to finish the bar. \n",
        "\n",
        "The bar is then rated against the given artist's lyrics with rhyme index, uniquness, and comprehension as the metrics.\n",
        "\n",
        "We want to generate bars with similar rhyme indicies and comprehension scores as the artist, but we want to ensure the bars are unique. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACXiwcv7UGKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_rap(model, artists_bars, length_of_bar=10, length_of_rap=20, min_score_threshold=-0.2, max_score_threshold=0.2, tries=5):\n",
        "  artists_avg_readability = calc_readability(artists_bars)\n",
        "  artists_avg_rhyme_idx = calc_rhyme_density(artists_bars)\n",
        "  fire_rap = []\n",
        "  cur_tries = 0\n",
        "  candidate_bars = []\n",
        "\n",
        "  while len(fire_rap) < length_of_rap:\n",
        "    seed_phrase = markov_model.make_sentence(tries=500).split(\" \")\n",
        "    seed_phrase = \" \".join(seed_phrase[:3])\n",
        "    cur_tries += 1\n",
        "    bar = generate_bar(seed_phrase, model, rand.randrange(4, length_of_bar))\n",
        "    bar_score = score_bar(bar, artist_lyrics, artists_avg_readability, artists_avg_rhyme_idx) \n",
        "    candidate_bars.append((bar_score, bar))\n",
        "\n",
        "    if bar_score <= max_score_threshold and bar_score >= min_score_threshold:\n",
        "      fire_rap.append(bar)\n",
        "      cur_tries = 0\n",
        "      print(\"Generated Bar: \", len(fire_rap))\n",
        "\n",
        "    if cur_tries >= tries:\n",
        "      lowest_score = np.Infinity\n",
        "      best_bar = \"\"\n",
        "      for bar in candidate_bars:\n",
        "        if bar[0] < lowest_score:\n",
        "          best_bar = bar[1]\n",
        "          candidate_bars = []\n",
        "      \n",
        "      print(\"Generated Bar: \", len(fire_rap))\n",
        "      fire_rap.append(best_bar)\n",
        "      cur_tries = 0\n",
        "      \n",
        "  print(\"Generated rap with avg rhyme density: \", calc_rhyme_density(fire_rap), \"and avg readability of: \", calc_readability(fire_rap))\n",
        "  return fire_rap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNCR2T-E3tND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_bar(seed_phrase, model, length_of_bar):\n",
        "  for i in range(length_of_bar):\n",
        "    seed_tokens = pad_sequences(tokenizer.texts_to_sequences([seed_phrase]), maxlen=29)\n",
        "    output_p = model.predict(seed_tokens)\n",
        "    output_word = np.argmax(output_p, axis=1)[0]-1\n",
        "    seed_phrase += \" \" + str(list(tokenizer.word_index.items())[output_word][0])\n",
        "  return seed_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nq07_YChQ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_bars(input_bar, artists_bars):\n",
        "  '''\n",
        "    input_bars are the fire bars our AI generates\n",
        "    artists_bars are the original bars for the artist\n",
        "\n",
        "    The lower the score the better! We want unique bars\n",
        "  '''\n",
        "  # Converts sentences to matrix of token counts\n",
        "  avg_dist = 0\n",
        "  total_counted = 0\n",
        "  for bar in artists_bars:\n",
        "    v = CountVectorizer()\n",
        "    # Vectorize the sentences\n",
        "    word_vector = v.fit_transform([input_bar, bar])\n",
        "    # Compute the cosine distance between the sentence vectors\n",
        "    cos_dist = 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
        "    if not math.isnan(cos_dist):\n",
        "      avg_dist += 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
        "      total_counted += 1\n",
        "  return avg_dist/total_counted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg4BGq3sn3Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Rhyme density is calculated by taking the number of rhymed syllables and divide it by total number of syllables'''\n",
        "def calc_rhyme_density(bars):\n",
        "  total_syllables = 0\n",
        "  rhymed_syllables = 0\n",
        "  for bar in bars:\n",
        "    for word in bar.split():\n",
        "      p = pronouncing.phones_for_word(word)\n",
        "      if len(p) == 0:\n",
        "        break\n",
        "      syllables = pronouncing.syllable_count(p[0])\n",
        "      total_syllables += syllables\n",
        "      has_rhyme = False\n",
        "      for rhyme in pronouncing.rhymes(word):\n",
        "        if has_rhyme:\n",
        "          break\n",
        "        for idx, r_bar in enumerate(bars):\n",
        "          if idx > 4:\n",
        "            break\n",
        "          if rhyme in r_bar:\n",
        "            rhymed_syllables += syllables\n",
        "            has_rhyme = True\n",
        "            break\n",
        "  return rhymed_syllables/total_syllables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtYsdyMoskxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_readability(input_bars):\n",
        "  avg_readability = 0\n",
        "  for bar in input_bars:\n",
        "    avg_readability += textstat.automated_readability_index(bar)\n",
        "  return avg_readability / len(input_bars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt8BZx7WsPW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_bar(input_bar, artists_bars, artists_avg_readability, artists_avg_rhyme_idx):\n",
        "  gen_readability = textstat.automated_readability_index(input_bar)\n",
        "  gen_rhyme_idx = calc_rhyme_density(input_bar)\n",
        "  comp_bars = compare_bars(input_bar, artists_bars)\n",
        "\n",
        "  # Scores based off readability, rhyme index, and originality. The lower the score the better.\n",
        "  bar_score = (artists_avg_readability - gen_readability) + (artists_avg_rhyme_idx - gen_rhyme_idx) + comp_bars\n",
        "  return bar_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYggE8dvoz3l",
        "colab_type": "text"
      },
      "source": [
        "Finally, Generate four different rap songs utilizing the SimpleRNN, GRU, LSTM, and CNN+LSTM architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEAO7hc4DX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67cc3ed9-1dab-4227-9f77-ac96ce95c561"
      },
      "source": [
        "seed_sentence = markov_model.make_sentence(tries=100).split(\" \")\n",
        "seed_sentence = \" \".join(seed_sentence[:5])\n",
        "\n",
        "rnn = generate_rap(seed_sentence, rnn_model, artist_lyrics, length_of_bar = 8, tries=100)\n",
        "\n",
        "print(\"Rap Generated with SimpleRNN:\")\n",
        "for line in rnn:\n",
        "  print(line)\n",
        "print()\n",
        "\n",
        "gru = generate_rap(seed_sentence, gru_model, artist_lyrics, length_of_bar = 8, tries=100)\n",
        "\n",
        "print(\"Rap Generated with GRU:\")\n",
        "for line in gru:\n",
        "  print(line)\n",
        "print()\n",
        "\n",
        "lstm = generate_rap(seed_sentence, lstm_model, artist_lyrics, length_of_bar = 8, tries=100)\n",
        "\n",
        "print(\"Rap Generated with LSTM:\")\n",
        "for line in lstm:\n",
        "  print(line)\n",
        "print()\n",
        "\n",
        "cnn = generate_rap(seed_sentence, cnn_model, artist_lyrics, length_of_bar = 8, tries=100)\n",
        "\n",
        "print(\"Rap Generated with CNN+LSTM:\")\n",
        "for line in cnn:\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Bar:  1\n",
            "Generated Bar:  2\n",
            "Generated Bar:  3\n",
            "Generated Bar:  4\n",
            "Generated Bar:  5\n",
            "Generated Bar:  6\n",
            "Generated Bar:  7\n",
            "Generated Bar:  8\n",
            "Generated Bar:  9\n",
            "Generated Bar:  10\n",
            "Generated Bar:  11\n",
            "Generated Bar:  12\n",
            "Generated Bar:  13\n",
            "Generated Bar:  14\n",
            "Generated Bar:  15\n",
            "Generated Bar:  16\n",
            "Generated Bar:  17\n",
            "Generated Bar:  18\n",
            "Generated Bar:  19\n",
            "Generated Bar:  20\n",
            "Generated rap with avg rhyme density:  0.5030674846625767 and avg readability of:  2.0599999999999996\n",
            "Rap Generated with SimpleRNN:\n",
            "Now you're throwing me baby know it know\n",
            "Look I gotta started with you hook drake\n",
            "I swear it happened no tellin' yeah yeah\n",
            "Turn my birthday man low changed you\n",
            "I see em right now hook drake again things alone\n",
            "That new s*** teams rufio copy rose more\n",
            "I love the picture you know hook drake back back\n",
            "Yeah, but shout yeah yeah yeah yeah yeah\n",
            "Yeah I'm leaving, one time back dog life\n",
            "Woah, woah, heard woah yeah oh yeah yeah\n",
            "I got money away blow now tellin' yeah yeah yeah\n",
            "You know what know though know it out you nothing\n",
            "Yeah I'm leaving, one time back dog life\n",
            "Turn my birthday man low changed you\n",
            "You should see me baby good time you working girl\n",
            "Yeah I'm leaving, one time back dog life\n",
            "You not from something baby timeline yo me out me\n",
            "And how the low changed up last gas guitar thing\n",
            "I got money away blow now tellin' yeah yeah yeah\n",
            "Like I catch alone way hook up whoa this versace\n",
            "\n",
            "Generated Bar:  1\n",
            "Generated Bar:  2\n",
            "Generated Bar:  3\n",
            "Generated Bar:  4\n",
            "Generated Bar:  5\n",
            "Generated Bar:  6\n",
            "Generated Bar:  7\n",
            "Generated Bar:  8\n",
            "Generated Bar:  9\n",
            "Generated Bar:  10\n",
            "Generated Bar:  11\n",
            "Generated Bar:  12\n",
            "Generated Bar:  13\n",
            "Generated Bar:  14\n",
            "Generated Bar:  15\n",
            "Generated Bar:  16\n",
            "Generated Bar:  17\n",
            "Generated Bar:  18\n",
            "Generated Bar:  19\n",
            "Generated Bar:  20\n",
            "Generated rap with avg rhyme density:  0.5176470588235295 and avg readability of:  1.9449999999999998\n",
            "Rap Generated with GRU:\n",
            "That's why I died everything big crazy on me\n",
            "Who keepin' score up yeah yeah yeah yeah\n",
            "I've loved and you everything big crazy on me on\n",
            "I want this stone another drink away\n",
            "Same **** that go 'head go 'head\n",
            "Looking for a room room s villains bands\n",
            "I think them bun about blow her faded working to\n",
            "When I should dance begin feel that year bed\n",
            "But I'm about you hook drake on on camera camera\n",
            "I don't judge her happened out more\n",
            "Don't know where at all yeah yeah yeah yeah yeah\n",
            "I been trynna here yeah yeah yeah yeah yeah yeah\n",
            "Just know that go 'head go 'head 'head 'head\n",
            "I've been so amazing much good time\n",
            "I want this stone another drink away you\n",
            "I can tell baby things alone alone alone\n",
            "I want this stone another drink away\n",
            "What they got it poppin' to go there there there\n",
            "I want this stone another drink away you\n",
            "Passive with the outro tho my woes outro\n",
            "\n",
            "Generated Bar:  1\n",
            "Generated Bar:  2\n",
            "Generated Bar:  3\n",
            "Generated Bar:  4\n",
            "Generated Bar:  5\n",
            "Generated Bar:  6\n",
            "Generated Bar:  7\n",
            "Generated Bar:  8\n",
            "Generated Bar:  9\n",
            "Generated Bar:  10\n",
            "Generated Bar:  11\n",
            "Generated Bar:  12\n",
            "Generated Bar:  13\n",
            "Generated Bar:  14\n",
            "Generated Bar:  15\n",
            "Generated Bar:  16\n",
            "Generated Bar:  17\n",
            "Generated Bar:  18\n",
            "Generated Bar:  19\n",
            "Generated Bar:  20\n",
            "Generated rap with avg rhyme density:  0.3684210526315789 and avg readability of:  1.9749999999999996\n",
            "Rap Generated with LSTM:\n",
            "Get the **** lick alone same that wait now up\n",
            "****, see what uh huh heart thing up yeah\n",
            "Despite the things though up up up up yeah yeah\n",
            "You da best man road man where where where where\n",
            "Based on what yeah yeah yeah yeah yeah yeah yeah\n",
            "Guess she don't thing time here place know me go\n",
            "She say she love me over more everything\n",
            "You just know out go 'head 'head through\n",
            "This for y'all mouth thing time here\n",
            "They don't love her pills baby baby know\n",
            "****, see what uh huh heart thing up yeah\n",
            "But you don't chose it you side obligated paid g\n",
            "They don't love her pills baby baby know\n",
            "I think the kitchen be boy baby baby quick quick\n",
            "That can only one girl there ready thing up yeah\n",
            "I want this there head hoo something\n",
            "Crazy like all home home home home home home\n",
            "Then nothing's gonna ooh ooh ooh ooh ooh ooh\n",
            "Says a lot teams ones rocco destroy ones\n",
            "I want this there head hoo something\n",
            "\n",
            "Generated Bar:  1\n",
            "Generated Bar:  2\n",
            "Generated Bar:  3\n",
            "Generated Bar:  4\n",
            "Generated Bar:  5\n",
            "Generated Bar:  6\n",
            "Generated Bar:  7\n",
            "Generated Bar:  8\n",
            "Generated Bar:  8\n",
            "Generated Bar:  10\n",
            "Generated Bar:  11\n",
            "Generated Bar:  12\n",
            "Generated Bar:  13\n",
            "Generated Bar:  14\n",
            "Generated Bar:  15\n",
            "Generated Bar:  16\n",
            "Generated Bar:  17\n",
            "Generated Bar:  18\n",
            "Generated Bar:  19\n",
            "Generated Bar:  20\n",
            "Generated rap with avg rhyme density:  0.33519553072625696 and avg readability of:  2.2599999999999993\n",
            "Rap Generated with CNN+LSTM:\n",
            "They still out know play through now out out\n",
            "I got it dedicate dedicate you yeah\n",
            "I've been waiting much much aye aye days aye aye\n",
            "Saturday nights, off this here now now out on on\n",
            "I'm finna do big see myself nowhere true\n",
            "Look at my landed mergers alone baby\n",
            "It wouldn't be greatest money it out know ow\n",
            "I've been waiting much much aye aye days aye\n",
            "Oti, oti, there's change where where where where schemin'\n",
            "Looking for some skin find to out slowly\n",
            "I had to kidnappers away all bling call days\n",
            "After this drop down down solid did down\n",
            "Last year I swear hippy her lesson again set\n",
            "Im going through it body no tellin' more ya gone\n",
            "I've been waiting much much aye aye days aye\n",
            "I got a pills propaganda money too too know know\n",
            "Gotta get a minaj beginning to out out too state\n",
            "Take it to kidnappers money help to too too okay\n",
            "Tell me how ooh on back business yah along along\n",
            "Guess she don't tonight it it too rich night\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}